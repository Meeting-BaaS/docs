---
title: Artifacts
description: Complete guide to all artifacts generated by Meeting BaaS v2 bots
icon: FileArchive
---

Meeting BaaS v2 generates various artifacts for each bot, including video recordings, audio files, transcriptions, and diarization data. All artifacts are stored securely and accessed via presigned URLs that are valid for 4 hours.

## Overview

When a bot completes recording a meeting, it generates several artifacts that you can access:

- **Video**: MP4 video recording (when video recording is enabled)
- **Audio**: FLAC audio recording (always generated)
- **Transcription**: Standardized transcription with accurate timestamps
- **Raw Transcription**: Provider-specific transcription with advanced features
- **Diarization**: Speaker identification and timing data

All artifacts are accessed via presigned URLs provided in bot details responses and webhook payloads.

## Video Artifact

The video artifact contains the complete video recording of the meeting.

**Format**: MP4 video file

**When Available**: Only when `recording_mode` is NOT `audio_only`

**Use Cases**:

- Providing users with raw video recording of meetings
- Video playback and review
- Integration with video analysis tools
- Archival purposes
- Creating video summaries or highlights

**Access**: Available via the `video` field in bot details and webhook payloads. Returns `null` if video recording was not enabled or if the bot's data has been deleted.

**Signed URL**: Valid for 4 hours

## Audio Artifact

The audio artifact contains the complete audio recording of the meeting.

**Format**: FLAC audio file

**When Available**: Always generated for every bot (regardless of recording mode)

**Use Cases**:

- Audio-only playback
- Integration with external transcription workflows
- Audio analysis and processing
- Backup audio source
- Creating audio-only versions of meetings

**Access**: Available via the `audio` field in bot details and webhook payloads. Returns `null` if the bot's data has been deleted.

**Signed URL**: Valid for 4 hours

## Transcription Artifact

The transcription artifact provides a standardized, processed transcription with accurate timestamps and speaker identification.

**Format**: JSON file

**Structure**:

```json
{
  "bot_id": "123e4567-e89b-12d3-a456-426614174000",
  "provider": "gladia",
  "result": {
    "utterances": [
      {
        "text": "Hello everyone, welcome to the meeting",
        "language": "en",
        "start": 0.5,
        "end": 2.1,
        "confidence": 0.95,
        "channel": 0,
        "words": [
          {
            "word": "Hello",
            "start": 0.5,
            "end": 0.8,
            "confidence": 0.98
          },
          {
            "word": "everyone",
            "start": 0.8,
            "end": 1.2,
            "confidence": 0.96
          }
        ],
        "speaker": "John Doe"
      }
    ],
    "languages": ["en"],
    "total_utterances": 150,
    "total_duration": 3600.5
  },
  "created_at": "2025-01-15T11:00:00Z"
}
```

**Utterance Fields**:

- `text`: The transcribed text for this utterance
- `language`: ISO 639-1 language code (e.g., "en", "es")
- `start`: Start time in seconds (floating point)
- `end`: End time in seconds (floating point)
- `confidence`: Confidence score (0.0 to 1.0)
- `channel`: Audio channel number
- `words`: Array of word-level timestamps
- `speaker`: Real speaker name

**Word Structure**:

- `word`: The word text
- `start`: Start time in seconds
- `end`: End time in seconds
- `confidence`: Confidence score (0.0 to 1.0)

**Key Features**:

- Speaker names are real names (not numeric IDs)
- All utterances sorted chronologically
- Accurate timestamps for the entire meeting
- Word-level timestamps for precise alignment

**Use Cases**:

- Standard transcript display with accurate timestamps
- Building transcript viewers/players
- Search and indexing
- Meeting summaries and analysis
- Creating subtitles or captions
- Speaker analytics and talk time analysis

**Access**: Available via the `transcription` field in bot details and webhook payloads. Returns `null` if transcription was not enabled or if the bot's data has been deleted.

**Signed URL**: Valid for 4 hours

## Raw Transcription Artifact

The raw transcription artifact contains the complete, unmodified response from the transcription provider. This includes all provider-specific features and metadata.

**Format**: JSON file

**Structure**: Varies by transcription provider

**Important Notes**:

- Structure varies by provider
- Timestamps may not be accurate due to internal chunking processes
- Speaker IDs are numeric (not real names)
- Contains provider-specific features (summarization, LLM responses, etc.)

**Use Cases**:

- Accessing provider-specific features (summarization, LLM prompts, etc.)
- Custom transcription processing workflows
- Accessing full transcript text without time-matched utterances
- Integration with provider-specific APIs
- Accessing advanced features like translations, sentiment analysis, and named entity recognition

**Limitations**:

- NOT suitable for transcript display (use `transcription` artifact instead)
- Timestamps may not be accurate
- Speaker IDs are numeric, not names

**Access**: Available via the `raw_transcription` field in bot details and webhook payloads. Returns `null` if transcription was not enabled or if the bot's data has been deleted.

**Signed URL**: Valid for 4 hours

### Provider-Specific Structures

#### Gladia

The Gladia raw transcription combines all audio chunk transcriptions into a single file:

```json
{
  "bot_id": "123e4567-e89b-12d3-a456-426614174000",
  "transcriptions": [
    {
      "metadata": {
        "audio_duration": 1800.5,
        "number_of_distinct_channels": 1,
        "billing_time": 1800.5,
        "transcription_time": 22.1
      },
      "transcription": {
        "full_transcript": "Hello everyone, welcome to the meeting...",
        "languages": ["en"],
        "utterances": [
          {
            "start": 0.5,
            "end": 2.1,
            "confidence": 0.95,
            "channel": 0,
            "speaker": 0,
            "words": [
              {
                "word": "Hello",
                "start": 0.5,
                "end": 0.8,
                "confidence": 0.98
              }
            ],
            "text": "Hello everyone",
            "language": "en"
          }
        ]
      },
      "summarization": {
        "summary": "Meeting discussed Q4 goals and team alignment..."
      },
      "translation": {
        "es": {
          "utterances": [...]
        }
      },
      "audio_to_llm": {
        "response": "The meeting covered..."
      }
    },
    {
      "metadata": {
        "audio_duration": 1800.0,
        "number_of_distinct_channels": 1,
        "billing_time": 1800.0,
        "transcription_time": 23.0
      },
      "transcription": {
        "full_transcript": "Let's continue with the next topic...",
        "languages": ["en"],
        "utterances": [...]
      }
    }
  ],
  "created_at": "2025-01-15T11:00:00Z"
}
```

**Structure**:

- `bot_id`: The bot UUID
- `transcriptions`: Array of transcription payloads, one per audio chunk
- `created_at`: ISO 8601 timestamp when the combined transcription was created

Each element in the `transcriptions` array contains the Gladia transcription payload structure with:

**Additional Fields** (based on custom parameters):

- `summarization`: AI-generated meeting summary
- `translation`: Translated transcriptions by target language
- `sentiment_analysis`: Sentiment scores for utterances
- `named_entity_recognition`: Extracted entities (names, organizations, locations)
- `audio_to_llm`: LLM prompt responses
- `moderation`: Content moderation results
- `chapterization`: Automatic meeting chapters
- And more based on your configuration

**Reference**: For complete documentation on Gladia's response structure and all available fields, see the [Gladia API Documentation](https://docs.gladia.io/api-reference/v2/pre-recorded/callback/success).

#### Additional Providers

Support for additional transcription providers (Assembly AI, Deepgram, etc.) is coming soon. Each provider will have its own structure documented here.

## Diarization Artifact

The diarization artifact contains speaker identification and timing information, useful for custom transcription workflows.

**Format**: JSONL file

**Structure** (Zoom example):

```jsonl
{"speaker": "John Doe", "start_time": 0.5, "end_time": 5.2, "user_id": 123, "lang": "en"}
{"speaker": "Jane Smith", "start_time": 5.3, "end_time": 10.1, "user_id": 456, "lang": "en"}
```

**Structure** (Google Meet/Teams example):

```jsonl
{"speaker": "John Doe", "start_time": 0.5, "end_time": 5.2, "user_id": 1}
{"speaker": "Jane Smith", "start_time": 5.3, "end_time": 10.1, "user_id": 2}
```

**Fields**:

- `speaker`: Real speaker name
- `start_time`: Start time in seconds
- `end_time`: End time in seconds
- `user_id`: Platform or Assigned user ID - (Zoom and Google Meet only, optional)
- `lang`: Language code (Zoom only, optional)

**Use Cases**:

- Custom transcription workflows
- Speaker identification and analysis
- Building custom transcript processing
- Integration with external diarization tools
- Maintaining speaker-to-transcript relationships
- Talk time analysis per speaker

**Access**: Available via the `diarization` field in bot details and webhook payloads. Returns `null` if diarization data is not available or if the bot's data has been deleted.

**Signed URL**: Valid for 4 hours

**Platform Differences**:

- **Zoom**: Includes `user_id` (platform user ID) and optional `lang` fields in each segment
- **Google Meet/Teams**: Includes `user_id` (assigned user ID) when available. The assigned ID attempts to remain consistent even if a participant rejoins the meeting.

## Additional Response Fields

### Transcription IDs

**Type**: `string[] | null`

**Description**: Array of transcription job IDs from the transcription provider

**Use Cases**:

- BYOK (Bring Your Own Key) users maintaining relationships with transcription providers
- Accessing provider-specific endpoints using these IDs
- Tracking transcription jobs across provider APIs
- Debugging and support
- Correlating transcription errors with specific provider jobs

**Example**:

```json
{
  "transcription_ids": ["gladia-job-12345", "gladia-job-12346"],
  "transcription_provider": "gladia"
}
```

**Access**: Available via the `transcription_ids` field in bot details and webhook payloads. Returns `null` if transcription was not enabled.

### Participants Array

**Type**: `Array<{name: string, id: number | null, display_name?: string, profile_picture?: string}>`

**Description**: List of all participants who joined the meeting

**Structure**:

```json
{
  "participants": [
    {
      "name": "John Doe",
      "id": 1,
      "display_name": "John",
      "profile_picture": "https://lh3.googleusercontent.com/..."
    },
    {
      "name": "Jane Smith",
      "id": 2
    }
  ]
}
```

**Fields**:

- `name`: Participant full name
- `id`: Platform or assigned user ID (null if unavailable)
- `display_name`: Display name shown in UI (optional, only present if different from `name`)
- `profile_picture`: Profile picture URL (optional, only present when available)

**Use Cases**:

- Participant tracking and analytics
- Meeting attendance reports
- Integration with CRM/HR systems
- Building participant lists for meeting summaries

**Access**: Available via the `participants` field in bot details and webhook payloads.

### Speakers Array

**Type**: `Array<{name: string, id: number | null, display_name?: string, profile_picture?: string}>`

**Description**: List of speakers identified in the meeting (subset of participants who spoke)

**Structure**: Same as participants array

**Fields**:

- `name`: Speaker full name
- `id`: Platform or assigned user ID (null if unavailable)
- `display_name`: Display name shown in UI (optional, only present if different from `name`)
- `profile_picture`: Profile picture URL (optional, only present when available)

**Use Cases**:

- Speaker analytics
- Talk time analysis
- Identifying active participants
- Building speaker-focused meeting summaries

**Access**: Available via the `speakers` field in bot details and webhook payloads.

## Artifact Access

All artifacts are accessed via presigned URLs that are valid for 4 hours from the time they are generated.

**Where to Find Artifacts**:

- `GET /v2/bots/{bot_id}` response - All artifact URLs in the response
- `bot.completed` webhook payload - All artifact URLs when bot completes
- Bot callbacks - Same URLs as webhook payloads

**Important Notes**:

- URLs expire after 4 hours - download artifacts promptly
- If `artifacts_deleted: true`, all artifact URLs will be `null`
- Artifacts are stored securely with data retention tags
- Download and store artifacts in your own storage for long-term access

## Data Retention and Security

With security built into every aspect of v2, data retention is as well. This approach severely limits data exposure by automatically removing artifacts after a specified retention period.

### Retention by Plan

Data retention periods vary by your API plan:

- **Pay-as-you-go**: 3 days
- **Pro**: 7 days
- **Scale**: 14 days
- **Enterprise**: 30 days

### Automatic Deletion

A background job automatically deletes artifacts that have exceeded the retention period based on your plan. This ensures data is not stored longer than necessary and minimizes security exposure.

### Manual Deletion

We recommend users take ownership of their data by using the `DELETE /v2/bots/{bot_id}/delete-data` endpoint to manually delete artifacts once they've been saved at your end.

**What Gets Deleted**:

- All artifacts from S3 (video, audio, transcription, diarization, screenshots)
- Optionally deletes transcription data from the transcription provider (default: `true`)
- Sets `artifacts_deleted: true` flag
- Ensures complete data scrubbing from our system

**Transcription Provider Deletion**: When using the delete endpoint with `delete_from_provider=true` (default), we also delete the transcription data from the transcription provider's servers (e.g., Gladia). This ensures complete data removal across all systems.

### Extended Retention

If you have a specific reason for needing data retained longer than your plan's default retention period, please [contact our support team](https://dashboard.meetingbaas.com/support-center) and we'll discuss how we can best support your needs.

## Platform-Specific Notes

### Zoom

- JSONL diarization format
- User ID mapping in diarization data
- Multi-speaker support with channel-based audio

### Google Meet

- JSONL diarization format (same as Zoom)
- Network-based speaker detection for improved accuracy
- Diarization segments include `speaker`, `start_time`, `end_time`, and `user_id` (assigned user ID)
- Single audio file processing
- **Diarization Accuracy**: 
  - Uses network-based detection which provides more accurate speaker identification
  - May have slight inaccuracies in timestamp alignment (typically less than 1 second)
  - Our system uses a statistical analysis approach to improve accuracy:
    - Matches transcription utterances to diarization segments using a Â±1 second time window
    - Samples 30% of utterances (minimum 50, maximum 200 samples per speaker) for statistical significance
    - Uses frequency-based matching: the speaker with the highest match count within the time window is selected
    - Calculates confidence scores based on match frequency
    - This approach compensates for any timing discrepancies by finding the most likely speaker match statistically
  - For the most accurate timestamps, use the `transcription` artifact (output transcription) which applies timestamp offsets to account for chunk boundaries and provides accurate timing across the entire meeting

### Microsoft Teams

- JSONL diarization format (same as Zoom)
- UI-based speaker detection
- Diarization segments only include `speaker`, `start_time`, and `end_time` (no `user_id` or `lang`)
- Single audio file processing

## Best Practices

1. **Download Promptly**: Presigned URLs expire after 4 hours - download artifacts as soon as they're available
2. **Store Long-Term**: If you need artifacts long-term, download and store them in your own storage
3. **Use Webhooks**: Set up webhooks to receive artifact URLs automatically when bots complete
4. **Delete When Done**: Use the delete endpoint to remove data when you no longer need it
5. **Use Appropriate Artifacts**: Use `transcription` for display, `raw_transcription` for advanced features
6. **Track Transcription IDs**: For BYOK users, use `transcription_ids` to correlate with provider jobs

## Examples

### Accessing Artifacts from Bot Details

```bash
curl -X GET "https://api.meetingbaas.com/v2/bots/BOT-ID" \
     -H "x-meeting-baas-api-key: YOUR-API-KEY"
```

**Response**:

```json
{
  "success": true,
  "data": {
    "bot_id": "123e4567-e89b-12d3-a456-426614174000",
    "status": "completed",
    "video": "https://s3.amazonaws.com/.../video.mp4",
    "audio": "https://s3.amazonaws.com/.../output.flac",
    "transcription": "https://s3.amazonaws.com/.../output_transcription.json",
    "raw_transcription": "https://s3.amazonaws.com/.../raw_transcription.json",
    "diarization": "https://s3.amazonaws.com/.../diarization.jsonl",
    "participants": [
      { "name": "John Doe", "id": 1, "display_name": "John", "profile_picture": "https://lh3.googleusercontent.com/..." },
      { "name": "Jane Smith", "id": 2 }
    ],
    "speakers": [
      { "name": "John Doe", "id": 1, "display_name": "John", "profile_picture": "https://lh3.googleusercontent.com/..." },
      { "name": "Jane Smith", "id": 2 }
    ],
    "transcription_ids": ["gladia-job-12345"],
    "transcription_provider": "gladia"
  }
}
```

### Accessing Artifacts from Webhook

When a bot completes, you'll receive a webhook with all artifact URLs:

```json
{
  "event": "bot.completed",
  "data": {
    "bot_id": "123e4567-e89b-12d3-a456-426614174000",
    "video": "https://s3.amazonaws.com/.../video.mp4",
    "audio": "https://s3.amazonaws.com/.../output.flac",
    "transcription": "https://s3.amazonaws.com/.../output_transcription.json",
    "raw_transcription": "https://s3.amazonaws.com/.../raw_transcription.json",
    "diarization": "https://s3.amazonaws.com/.../diarization.jsonl",
    "transcription_ids": ["gladia-job-12345"],
    "transcription_provider": "gladia"
  }
}
```

## Frequently Asked Questions

<Accordions>
  <Accordion title="How does Meeting BaaS create transcript chunks?">
    Meeting BaaS uses different chunking strategies depending on the meeting platform:

    **Zoom Bot:**

    - Creates **speaker-based chunks** in real-time as audio is received from the Zoom SDK
    - Each chunk is associated with a specific `user_id` (speaker)
    - Chunks are created automatically based on SDK audio events (approximately every 5-6 minutes per speaker)
    - Minimum chunk size: 100KB (smaller chunks are skipped)
    - Each chunk maintains speaker identity throughout, enabling perfect diarization

    **Google Meet/Teams Bot:**

    - Creates **time-based chunks** after the meeting recording is complete
    - The entire meeting audio is first recorded, then split into chunks using FFmpeg
    - Maximum chunk duration: 2 hours (7,200 seconds) per chunk
    - Chunks are created sequentially based on time intervals, not speaker identity
    - This approach allows processing of very long meetings while respecting transcription provider limits

    The chunking strategy ensures optimal transcription quality while managing file sizes and processing time efficiently.

  </Accordion>

  <Accordion title="How accurate are the diarization timestamps in the diarization file?">
    Diarization timestamp accuracy varies by platform:

    **Zoom:**

    - **Highly accurate** - Uses Zoom's native diarization with perfect speaker-to-user mapping
    - Timestamps are precise and directly tied to Zoom's audio stream
    - Each segment includes `user_id` for reliable speaker identification

    **Google Meet:**

    - Uses network-based speaker detection for improved accuracy
    - May have slight inaccuracies in timestamp alignment (typically less than 1 second)
    - See the [Google Meet section](#google-meet) above for detailed information about our statistical analysis approach

    **Microsoft Teams:**

    - **May have slight inaccuracies** - Uses UI-based speaker detection which can introduce latency
    - UI diarization relies on visual indicators (speaker highlighting) which may lag behind actual speech by 1-2 seconds

    For the most accurate timestamps, use the `transcription` artifact (output transcription) which applies timestamp offsets to account for chunk boundaries and provides accurate timing across the entire meeting.

  </Accordion>

  <Accordion title="How does Google Meet maintain consistent user_id across rejoins?">
    Google Meet uses a stable identifier system to assign `user_id` values that remain consistent even when participants rejoin:

    - **Identification Method**: Creates a unique hash based on the participant's profile picture URL (preferred) or full name
    - **Consistency**: The `user_id` will remain the same as long as:
      - The participant's profile picture URL doesn't change between rejoins (highly unlikely)
      - Or, if no profile picture is available, the participant's name remains unchanged
    - **Why This Works**: Profile picture URLs are unique per Google account and rarely change, making them a stable identifier. If a profile picture isn't available, the full name serves as a fallback identifier.

    This approach ensures that the same participant receives the same `user_id` throughout the meeting, even if they temporarily leave and rejoin, making it easier to track speaker continuity in your analysis.

  </Accordion>

<Accordion title="How long are signed URLs valid?">
  All artifact signed URLs are valid for **4 hours** from the time they are
  generated. After 4 hours, the URLs expire and you'll need to fetch new URLs
  from the bot details endpoint or webhook.
</Accordion>

<Accordion title="What happens if I don't download artifacts before they expire?">
  If a signed URL expires, you can retrieve new signed URLs by calling `GET
  /v2/bots/{bot_id}`. The endpoint will generate fresh signed URLs that are
  valid for another 4 hours. However, if the artifacts have been deleted (either
  manually or after the retention period), the URLs will return `null`.
</Accordion>

  <Accordion title="What's the difference between transcription and raw_transcription?">
    - **`transcription`** (output transcription): A standardized, processed transcription with accurate timestamps, real speaker names, and word-level timestamps. This is the recommended artifact for displaying transcripts to users.

    - **`raw_transcription`**: The complete, unmodified response from the transcription provider. Contains provider-specific features (summarization, LLM responses, translations, etc.) but timestamps may not be accurate due to chunking, and speaker IDs are numeric rather than names.

    Use `transcription` for display purposes and `raw_transcription` when you need access to provider-specific advanced features.

  </Accordion>

  <Accordion title="Why are some artifact URLs null?">
    Artifact URLs can be `null` for several reasons:

    - **Video**: `null` when `recording_mode` is `audio_only` or if the artifact has been deleted
    - **Transcription/Raw Transcription**: `null` when transcription was not enabled or if artifacts have been deleted
    - **Diarization**: `null` when diarization data is not available or has been deleted
    - **All artifacts**: `null` when `artifacts_deleted: true` (data has been manually deleted or exceeded retention period)

  </Accordion>

  <Accordion title="How do I know when artifacts are ready?">
    Artifacts are ready when the bot status is `completed`. You can:

    1. **Poll the bot details endpoint**: `GET /v2/bots/{bot_id}` - Check the `status` field
    2. **Use webhooks**: Set up a `bot.completed` webhook to receive artifact URLs automatically when the bot finishes
    3. **Check artifact URLs**: When `status: "completed"`, all available artifact URLs will be populated (non-null)

  </Accordion>

<Accordion title="Can I regenerate signed URLs after they expire?">
  Yes! Simply call `GET /v2/bots/{bot_id}` again to get fresh signed URLs. As
  long as the artifacts haven't been deleted, you'll receive new 4-hour valid
  URLs.
</Accordion>

  <Accordion title="What happens to artifacts after the retention period?">
    Artifacts are automatically deleted by a background job after your plan's retention period expires:

    - **Pay-as-you-go**: 3 days
    - **Pro**: 7 days
    - **Scale**: 14 days
    - **Enterprise**: 30 days

    After deletion, all artifact URLs will return `null` and `artifacts_deleted` will be `true`. We recommend downloading and storing artifacts in your own storage if you need long-term access.

  </Accordion>
</Accordions>

## Next Steps

- Learn about [Getting the Data](/docs/api-v2/getting-started/getting-the-data) to access artifacts
- Set up [Webhooks](/docs/api-v2/webhooks) to receive artifact URLs automatically
- Explore [Transcription](/docs/api-v2/transcription) features and custom parameters
- Check the [API Reference](/docs/api-v2/reference) for complete endpoint documentation
